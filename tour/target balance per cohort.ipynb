{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ce9374-515b-4f1b-827c-dddba7e455f3",
   "metadata": {},
   "source": [
    "### [**Responsible AI Mitigation Library**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/)\n",
    "\n",
    "<details> \n",
    "<summary>Encoders</summary>\n",
    "  \n",
    "  [**Link to Encoders**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/encoder/encoder.html)\n",
    "    The Encoders API allows for ordinal or one-hot encoding of categorical features.\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.EncoderOrdinal(df: Optional[Union[DataFrame, ndarray]] = None, col_encode: Optional[list] = None, categories: Union[dict, str] = 'auto', unknown_err: bool = False, unknown_value: Union[int, float] = -1, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**One Hot Encoding**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/encoder/ordinal.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.EncoderOHE(df: Optional[Union[DataFrame, ndarray]] = None, col_encode: Optional[list] = None, drop: bool = True, unknown_err: bool = True, verbose: bool = True\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "<details> \n",
    "<summary>Feature Selection</summary>\n",
    "\n",
    "[Link to Feature Selection](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/feat_sel/feat_sel.html)\n",
    "The Feature Selection API enables selecting a subset of features that are the most informative for the prediction task.\n",
    "\n",
    "  \n",
    "  [**Sequential Feature Selection**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/feat_sel/seq.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.SeqFeatSelection(df: Optional[Union[DataFrame, ndarray]] = None, label_col: Optional[str] = None, X: Optional[Union[DataFrame, ndarray]] = None, y: Optional[Union[DataFrame, ndarray]] = None, transform_pipe: Optional[list] = None, in_place: bool = False, regression: Optional[bool] = None, estimator: Optional[BaseEstimator] = None, n_feat: Union[int, str, tuple] = 'best', fixed_cols: Optional[list] = None, cv: int = 3, scoring: Optional[str] = None, forward: bool = True, save_json: bool = False, json_summary: str = 'seq_feat_summary.json', n_jobs: int = 1, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Feature Selection via Gradient Boosting**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/feat_sel/catboost.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.CatBoostSelection(df: Optional[Union[DataFrame, ndarray]] = None, label_col: Optional[str] = None, X: Optional[Union[DataFrame, ndarray]] = None, y: Optional[Union[DataFrame, ndarray]] = None, transform_pipe: Optional[list] = None, regression: Optional[bool] = None, estimator: Optional[Union[CatBoostClassifier, CatBoostRegressor]] = None, in_place: bool = False, catboost_log: bool = True, catboost_plot: bool = False, test_size: float = 0.2, cat_col: Optional[list] = None, n_feat: Optional[int] = None, fixed_cols: Optional[list] = None, algorithm: str = 'loss', steps: int = 1, save_json: bool = False, json_summary: str = 'cb_feat_summary.json', verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Feature Selection via Removing Correlated Features**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/feat_sel/correlation.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.CorrelatedFeatures(df: Optional[Union[DataFrame, ndarray]] = None, label_col: Optional[str] = None, X: Optional[Union[DataFrame, ndarray]] = None, y: Optional[Union[DataFrame, ndarray]] = None, transform_pipe: Optional[list] = None, in_place: bool = False, cor_features: Optional[list] = None, method_num_num: list = ['spearman'], num_corr_th: float = 0.85, num_pvalue_th: float = 0.05, method_num_cat: str = 'model', levene_pvalue: float = 0.01, anova_pvalue: float = 0.05, omega_th: float = 0.9, jensen_n_bins: Optional[int] = None, jensen_th: float = 0.8, model_metrics: list = ['f1', 'auc'], metric_th: float = 0.9, method_cat_cat: str = 'cramer', cat_corr_th: float = 0.85, cat_pvalue_th: float = 0.05, tie_method: str = 'missing', save_json: bool = True, json_summary: str = 'summary.json', json_corr: str = 'corr_pairs.json', json_uncorr: str = 'uncorr_pairs.json', compute_exact_matches: bool = True, verbose: bool = True)\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "<details> \n",
    "<summary>Imputers</summary>\n",
    "\n",
    "[Link to Imputers](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/imputer/imputer.html)\n",
    "The Imputer API enables a simple approach for replacing missing values across several columns with different parameters, simultaneously replacing with the mean, median, most constant, or most frequent value in a dataset.\n",
    "\n",
    "  \n",
    "  [**Basic Imputation**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/imputer/basic.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.BasicImputer(df: Optional[Union[DataFrame, ndarray]] = None, col_impute: Optional[list] = None, categorical: Optional[dict] = None, numerical: Optional[dict] = None, specific_col: Optional[dict] = None, verbose: bool = True)\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "<details> \n",
    "<summary>Sampling</summary>\n",
    "\n",
    "[Link to Sampling](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/sampler/sampler.html)\n",
    "The Sampling API enables data augmentation by rebalancing existing data or synthesizing new data.\n",
    "\n",
    "\n",
    "  [**Data Rebalance**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/sampler/rebalance.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.Rebalance(df: Optional[Union[DataFrame, ndarray]] = None, rebalance_col: Optional[str] = None, X: Optional[Union[DataFrame, ndarray]] = None, y: Optional[Union[DataFrame, ndarray]] = None, transform_pipe: Optional[list] = None, in_place: bool = False, cat_col: Optional[list] = None, strategy_over: Optional[Union[str, dict, float]] = None, k_neighbors: int = 4, over_sampler: Union[BaseSampler, bool] = True, strategy_under: Optional[Union[str, dict, float]] = None, under_sampler: Union[BaseSampler, bool] = False, n_jobs: int = 1, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Data Synthesis**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/sampler/synthesizer.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.Synthesizer(df: Optional[DataFrame] = None, label_col: Optional[str] = None, X: Optional[DataFrame] = None, y: Optional[DataFrame] = None, transform_pipe: Optional[list] = None, in_place: bool = False, model: Union[BaseTabularModel, str] = 'ctgan', epochs: int = 50, save_file: Optional[str] = None, load_existing: bool = True, verbose: bool = True)\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "<details> \n",
    "<summary>Scalers</summary>\n",
    "\n",
    "[Link to Scalers](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/scaler.html)\n",
    "The Scaler API enables applying numerical scaling transformations to several features at the same time.\n",
    "\n",
    "  \n",
    "  [**Data Standardization Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/standard.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataStandardScaler(scaler_obj: Optional[StandardScaler] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Min Max Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/minmax.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataMinMaxScaler(scaler_obj: Optional[MinMaxScaler] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Quantile Transformer Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/quantile.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataQuantileTransformer(scaler_obj: Optional[QuantileTransformer] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Power Transformer Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/power.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataPowerTransformer(scaler_obj: Optional[PowerTransformer] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Robust Statistics Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/robust.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataRobustScaler(scaler_obj: Optional[RobustScaler] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True))\n",
    "\n",
    "\n",
    "  ```\n",
    "  [**Data Normalization Scaling**](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/dataprocessing/scaler/normalize.html)\n",
    "\n",
    "  ```py\n",
    "  classdataprocessing.DataNormalizer(scaler_obj: Optional[Normalizer] = None, df: Optional[Union[DataFrame, ndarray]] = None, exclude_cols: Optional[list] = None, include_cols: Optional[list] = None, transform_pipe: Optional[list] = None, verbose: bool = True)\n",
    "  ```\n",
    "\n",
    "  \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcc486",
   "metadata": {},
   "source": [
    "# Assess income level predictions on adult census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca12889d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benushi\\Miniconda3\\envs\\tracker39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import raimitigations.dataprocessing as dp\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from lightgbm import LGBMClassifier\n",
    "import raimitigations.dataprocessing as dp\n",
    "from raimitigations.cohort import CohortDefinition, CohortManager\n",
    "from raimitigations.dataprocessing import EncoderOHE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfb10b8-5354-401b-b6c8-34914e511f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_value_counts_cohort(y_full, subsets, normalize = True, figname = 'fig.png'):\n",
    "    plt.figure().clear()\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18, 10)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    if normalize:\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "    value_count = y_full.value_counts(normalize=normalize)\n",
    "\n",
    "    subsets_col = ['full df', 'full df']\n",
    "    counts_col = [value_count[0], value_count[1]]\n",
    "    label_col = [0, 1]\n",
    "\n",
    "    for key in subsets.keys():\n",
    "        value_count = subsets[key][\"y\"].value_counts(normalize=normalize)\n",
    "        subsets_col += [key, key]\n",
    "        counts_col += [value_count[0], value_count[1]]\n",
    "        label_col += [0, 1]\n",
    "\n",
    "    count_df = pd.DataFrame({\"subsets\":subsets_col, \"label\":label_col, \"counts\":counts_col})\n",
    "    \n",
    "    y_label = \"Occurrences\"\n",
    "    if normalize:\n",
    "        y_label = \"Fraction\"\n",
    "\n",
    "    ax = sns.barplot(x=\"subsets\", y=\"counts\", hue=\"label\", data=count_df)\n",
    "    ax.set_xlabel(\"Cohorts\", fontsize=50)\n",
    "    ax.set_ylabel(y_label, fontsize=50)\n",
    "    ax.tick_params(labelsize=50)\n",
    "    plt.legend(fontsize=50)\n",
    "    plt.savefig(figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd4647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def split_label(dataset, target_feature):\n",
    "    X = dataset.drop([target_feature], axis=1)\n",
    "    y = dataset[[target_feature]]\n",
    "    return X, y\n",
    "\n",
    "target_feature = 'income'\n",
    "categorical_features = ['workclass', 'education', 'marital-status',\n",
    "                        'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "\n",
    "train_data = pd.read_csv('data/adult-train.csv', skipinitialspace=True, header=0)\n",
    "test_data = pd.read_csv('data/adult-test-sample.csv', skipinitialspace=True, header=0)\n",
    "\n",
    "X_train_original, y_train = split_label(train_data, target_feature)\n",
    "X_test_original, y_test = split_label(test_data, target_feature)\n",
    "\n",
    "estimator = LGBMClassifier(random_state=0, n_estimators=5)\n",
    "\n",
    "cohort_pipeline = [\n",
    "    dp.BasicImputer(verbose=False),\n",
    "    dp.EncoderOHE(unknown_err=True)\n",
    "]\n",
    "# Define the cohorts\n",
    "c1 = [ [ ['relationship', '==', 'Wife'], 'or', ['relationship', '==', 'Husband']]]\n",
    "c2 = None\n",
    "\n",
    "#24463 is the size of the majority class for c1\n",
    "c1_pipe = [dp.Rebalance(verbose=False,strategy_over={0:24463, 1:24463})]\n",
    "c2_pipe = []\n",
    "\n",
    "rebalance_cohort = CohortManager(\n",
    "    transform_pipe=[c1_pipe, c2_pipe],\n",
    "    cohort_def={\"married\":c1, \"not_married\":c2}\n",
    "    )\n",
    "    \n",
    "new_X, new_y = rebalance_cohort.fit_resample(X_train_original, y_train)\n",
    "\n",
    "#Create a pipeline that uses the cohort manager\n",
    "pipe = Pipeline([\n",
    "    (\"encoder\", dp.EncoderOHE(unknown_err=True)),\n",
    "    (\"model\", estimator)\n",
    "    ])\n",
    "\n",
    "pipe.fit(new_X, new_y)\n",
    "subsets = rebalance_cohort.get_subsets(new_X, new_y, apply_transform=False)\n",
    "plot_value_counts_cohort(new_y, subsets, normalize=True, figname='images/after_mitigation_rebalance_per_cohort_target.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca38e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(pipe, open(\"../rebalance_per_cohort_target\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
